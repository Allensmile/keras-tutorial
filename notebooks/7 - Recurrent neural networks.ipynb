{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "Like CNNs, which are tailored for processing 2D data, recurrent neural networks (RNNs) are designed for processing sequences or time-series by exploring the power of **recurrent connections**. Depending on the way they are constructed, RNNs can process sequences of variable length, which is not the case with feedforward nets (which have fixed input size) and CNNs (the same kernels can be reused for larger inputs, but usually the \"decision\" layer has fixed size).\n",
    "\n",
    "Similarly to CNNs, RNNs also have a lot of parameter sharing, but now parameters are shared across different time-steps.\n",
    "\n",
    "RNNs are currently part of the state-of-the-art techniques for acoustic modeling in ASR and natural language processing ([Google voice search](https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html) and many other Google products are powered by RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simple RNN\n",
    "\n",
    "<img src=\"RNN-unrolled.png\" width=650px>\n",
    "\n",
    "This type of RNN is available on Keras as `keras.layers.recurrent.SimpleRNN`.\n",
    "\n",
    "### Issues: vanishing and exploding gradients\n",
    "\n",
    "The reason simple RNNs are almost never used in practice is that they are very hard to optimize properly when learning long-term dependencies. Very small and very large gradients lead to the so-called *vanishing gradients* and *exploding gradients* issues, which happen when a small/large gradient is propagated across many timesteps, decreasing or increasing in an exponential fashion across the recurrent connections. Gated RNNs such as long short-term memory units (LSTM) and gated recurrent units (GRU) were proposed to solve these issues.\n",
    "\n",
    "## LSTM\n",
    "\n",
    "![](lstm.png)\n",
    "(image from Wikimedia Commons: https://commons.wikimedia.org/wiki/File:Long_Short_Term_Memory.png)\n",
    "\n",
    "This type of RNN is available on Keras as `keras.layers.recurrent.LSTM`.\n",
    "\n",
    "\n",
    "## GRU\n",
    "\n",
    "GRUs are another proposed solution for the problem of vanishing gradients, which use the same gating principle but are simpler than LSTMs as they only have two gates (reset and update) and its internal memory is the same as its hidden state, instead of using a separate cell like LSTMs. There are usually no big performance gaps between LSTM and GRU networks with the same number of parameters.\n",
    "\n",
    "GRUs are available on Keras as `keras.layers.recurrent.GRU`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient value and norm-clipping\n",
    "\n",
    "Another strategy that can mitigate the *exploding gradients* issue is gradient clipping. Gradients can be clipped either by their maximum absolute value or by the total L2 gradient norm. All optimizers in Keras support both modes of gradient clipping, by using the following keyword parameters:\n",
    "\n",
    "- `clipnorm=value` (value: float > 0): Gradients will be clipped when their L2 norm exceeds this value\n",
    "- `clipvalue=value` (value: float > 0): Gradients will be clipped when their L2 norm exceeds this value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to connect the output of a recurrent layer to another layer\n",
    "\n",
    "We can connect the output of an RNN to the next layer in a model in two different ways:\n",
    "\n",
    "- Use all the hidden states generated by the RNN (a sequence of feature vectors) for a given sequence\n",
    "- Use only the last hidden state (one feature vector per sequence)\n",
    "\n",
    "If using the first approach, the next layer has to support processing sequences (or you can flatten the sequence as a single vector, as we have seen in the CNN section of this tutorial). To choose whether you want a recurrent layer to output a sequence or a single vector, use the keyword parameter `return_sequences`. The default for this parameter is `False`.\n",
    "\n",
    "Besides recurrent layers, Keras also supports using any layer for processing a sequence by using the `TimeDistributed` wrapper. This wrapper is equivalent to making a copy of the wrapped layer for each timestep of the sequence, with all parameters tied. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3737e6da40a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Dense(256)))\n",
    "\n",
    "# or\n",
    "\n",
    "model.add(LSTM(256)) # Using default value for return_sequences, which is False\n",
    "model.add(Dense(256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNNs\n",
    "\n",
    "Bidirectional RNNs use two RNNs in parallel to process a sequence: one goes forward in time, the other backwards. The outputs of both RNNs are then concatenated and used as the input for the next layer in the network. In Keras, one can implement a bidirectional RNN by using the keyword argument `go_backwards` and then a `Merge` layer to concatenate both outputs. Note that this requires using the functional API instead of the sequential API, which we will not cover today (please check the [tutorial](http://keras.io/getting-started/functional-api-guide/) for the functional API in the official documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(50, 100)) # 50 timesteps with 100 features each\n",
    "\n",
    "h1_fw = GRU(256)(x)\n",
    "h1_bw = GRU(256, go_backwards=True)(x)\n",
    "\n",
    "h1 = Merge([h1_fw, h1_bw], mode='concat', concat_axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full example\n",
    "\n",
    "As a speech-related example, let's implement a simple model to predict the clean magnitude spectrum for speechs signals based on the noisy magnitude spectrum.\n",
    "\n",
    "(Note: this is not the best way to go around solving this task, but it's a simpler model to use as an example.)\n",
    "\n",
    "You will notice that compilation times for RNNs are usually much longer than for the other networks we have seen in this tutorial, especially when you backpropagate over a large number of timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# In case your RNN is backpropagating over a large number of timesteps, you might need to do this\n",
    "# (only when using the Theano backend)\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "\n",
    "# Loading data. Input is already standardized (mean = 0, standard deviation = 1)\n",
    "X = np.load('stft_data_noisy.npy') # shape is (n_samples, n_frames, n_frequency_bins)\n",
    "y = np.load('stft_data_clean.npy') # same as above, but scaled to the range [0, 1]\n",
    "\n",
    "X_train, X_test = X[:10000], X[10000:]\n",
    "y_train, y_test = X[:10000], X[10000:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Dense(y_train.shape[-1], activation='sigmoid')))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, batch_size=32, nb_epoch=100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More info/references\n",
    "\n",
    "- [Keras documentation for recurrent layers](http://keras.io/layers/recurrent/)\n",
    "\n",
    "- [A. Graves, A.-R. Mohamed, and G. Hinton, “Speech recognition with deep recurrent neural networks,”](www.cs.toronto.edu/~fritz/absps/RNN13.pdf) in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6645–6649.\n",
    "\n",
    "- [Chapter 10 of the Deep Learning book - Sequence Modeling: Recurrent and Recursive Nets](http://www.deeplearningbook.org/contents/rnn.html)\n",
    "\n",
    "- [A. Karpathy, \"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "- [C. Olah, \"Understanding LSTMs\"](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
