{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization algorithms\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### Gradient update equation\n",
    "\n",
    "$$\n",
    "\\hat{g} \\leftarrow \\frac{1}{m} \\nabla_{\\theta} \\left( \\sum_i L(f(x^{(i)}); \\theta), y^{(i)} \\right)\\\\\n",
    "\\theta \\leftarrow \\theta - \\epsilon \\hat{g}\n",
    "$$\n",
    "\n",
    "### Why stochastic?\n",
    "\n",
    "Batch gradient descent requires computing the gradients for the entire dataset at each iteration (e.g., setting the sum in the above equation to $N$, where $N$ is the number of samples in the dataset). This has a very high computational cost with large datasets. Conversely, we can compute gradients is to approximate it using a single sample at a time and update the model parameters $\\theta$ based on this gradient. However, by doing this we might be \"throwing away\" computational resources, as it's often more efficient to compute gradients for a few samples in parallel (using matrix-matrix operations instead of matrix-vector operations).\n",
    "\n",
    "Minibatch stochastic gradient descent (mostly called just stochastic gradient descent nowadays) explores the tradeoff between using larger batches to get more accurate gradient estimates and explore parallelism, while not needing to compute the output for the entire dataset for a single parameter update. Often the minibatch size is chosen considering hardware characteristics (e.g., GPUs have better performance when using power of two minibatch sizes due to the way matrix multiplies is performed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate decay\n",
    "\n",
    "Due to the noisy gradients obtained by SGD, we often cannot use a fixed learning rate to update the parameters of our model. A common practice is to start with a given learning rate and then decay it exponentially at every epoch until it reaches an arbitrary minimum, or keep it fixed for a few epochs before decreasing it. On Keras, learning rate decay is implemented as a callback (`keras.callbacks.LearningRateScheduler`), which gets as an argument a Python function `f(epoch)` which returns the learning rate to be used for that epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "Momentum introduces a *velocity* variable which makes the parameters keep moving at the direction gradients from previous iterations pointed to:\n",
    "\n",
    "$$\n",
    "v \\leftarrow \\alpha v - \\epsilon \\frac{1}{m} \\nabla_{\\theta} \\left( \\sum_i L(f(x^{(i)}); \\theta), y^{(i)} \\right)\\\\\n",
    "\\theta \\leftarrow \\theta - \\epsilon \\hat{g}\n",
    "$$\n",
    "\n",
    "where $ 0 \\leq \\alpha < 1$ controls how quickly the contributions of previous gradients exponentially decay.\n",
    "\n",
    "There are two algorithms which incorporate momentum: standard and Nesterov momentum. The difference between Nesterov momentum and standard momentum is where the gradient is evaluated: with Nesterov momentum, the gradient is computed at an interim point (the point we would go to if we did not update the velocity), while with standard momentum the gradient is computed at the current point (as shown in the equations above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive learning rate\n",
    "\n",
    "Learning rate is one of the most important hyperparameters to set for training a DNN as it has a significant impact on model performance. Also, often the choice of a single learning rate for all parameters in our model is not the best, as different parameters have different sensitivities. The adaptive learning rate approach uses separate learning rates for each parameter, and automaticallty adapts these rates during training (as it would be insane trying to find optimal values for each parameter on your own!).\n",
    "\n",
    "Keras includes several adaptive learning rate algorithms:\n",
    "\n",
    "- RMSprop\n",
    "- Adagrad\n",
    "- Adadelta\n",
    "- Adam\n",
    "- Nadam\n",
    "\n",
    "If in doubt, RMSprop and Adam with the default parameters are often a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Intro to gradient descent](http://cs231n.github.io/optimization-1/)\n",
    "- [Nice blog post about the \"evolution\" of optimizers and their characteristics](http://sebastianruder.com/optimizing-gradient-descent/index.html)\n",
    "- [Chapter 8 of the Deep Learning Book - Optimization for Training Deep Models](http://www.deeplearningbook.org/contents/optimization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
